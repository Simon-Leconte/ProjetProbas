{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Numérique : Câble sous-marin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anisse Id-Benaddi & Simon Leconte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce projet est d'estimer la longueur de câble nécessaire pour traverser une étendue d'eau dont la profondeur en chaque point nous est inconnue, lorsque le câble est posé sur le fond de l'étendue d'eau. On ne connaît que quelques profondeurs, qui correspondent à des observations ponctuelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On discrétise le plancher marin en $N$ points d'abscisse $x_0,...,x_N$ régulièrement espacés de $\\Delta$ et de profondeur $z(x_0),...,z(x_N)$. La longueur du plancher marin réelle est donc $l=\\sum_{i=1}^N \\sqrt{\\Delta^2+(z(x_i)-z(x_{i-1}))^2}$. Nous allons donc effectuer des simulations estimant la valeur des $z_i$. Une simulation nous donnera donc un vecteur $Z=(Z(x_0),...,Z(x_N))$. On pourra alors calculer $L$, d'espérance $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions théoriques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La loi des grands nombres nous autorise à estimer l'espérance conditionnelle par la moyenne empirique de simulations conditionnelles. En effet, si on effectue $n$ simulations indépendantes de la longueur du plancher océanique $L_1,L_2,...,L_n$, la loi des grands nombres nous indique que si les $L_i$ sont de mêmes lois et de carrés intégrables, la variable aléatoire $M_n = \\dfrac{L_1+L_2+...+L_n}{n}$ converge presque surement et en moyenne vers $\\mathbb{E}(L)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après le cours \"Probabilité IV\", si on prend $Z=(Z_1,...,Z_N)$ gaussien  d'espérance $(\\mu,...,\\mu)$ et de matrice de covariance $C$ définie positive, $Y=(Z(x_{j_1}),...,Z(x_{j_n}))$ et $X$ contenant les éléments de $Z$ non présents dans $Y$.\n",
    "$f_{X|Y=y}(\\vec{x}) = \\dfrac{1}{(2\\pi)^{\\frac{N-n+2}{2}}}*\\dfrac{1}{\\sqrt{\\det{CS_X}}} * \\exp (-\\dfrac{1}{2}(x-\\psi (y))^TCS_X^{-1}(x-\\psi (y)) )$ où $CS_X = C_X-C_{X,Y}C_Y^{-1}C_{Y,X}$ et $\\psi (y) = (\\mu,...,\\mu) - C_{X,Y}C_Y^{-1}(y-(\\mu,...,\\mu))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $Y$ gaussien de moyenne $(0,...,0)$ et de variance $1$, de composantes indépendantes. Alors $\\Phi_Y(u)=\\exp (-\\dfrac{1}{2}u^2)$. Si $Z=m+RY$ avec $R$ une matrice et $m$ un vecteur : $\\Phi_Z(u)=e^{i<u,m>}\\Phi_Y(R^Tu) = e^{i<u,m>}e^{-\\frac{1}{2}(R^TR)(uu^T)}$ et donc $f_Z(z) = \\mathcal{F}^{-1}(\\Phi_Z)(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, il suffit de simuler $Z=\\mu+RY$ où $Y$ est de loi $\\mathcal{G}(0,1)$ et où $R$ est la décomposition de Cholesky de la matrice de covariance de $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
