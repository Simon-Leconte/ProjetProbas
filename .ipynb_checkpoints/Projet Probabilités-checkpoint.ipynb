{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Numérique : Câble sous-marin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anisse Id-Benaddi & Simon Leconte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce projet est d'estimer la longueur de câble nécessaire pour traverser une étendue d'eau dont la profondeur en chaque point nous est inconnue, lorsque le câble est posé sur le fond de l'étendue d'eau. On ne connaît que quelques profondeurs, qui correspondent à des observations ponctuelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On discrétise le plancher marin en $N$ points d'abscisse $x_0,...,x_N$ régulièrement espacés de $\\Delta$ et de profondeur $z(x_0),...,z(x_N)$. La longueur du plancher marin réelle est donc $l=\\sum_{i=1}^N \\sqrt{\\Delta^2+(z(x_i)-z(x_{i-1}))^2}$. Nous allons donc effectuer des simulations estimant la valeur des $z_i$. Une simulation nous donnera donc un vecteur $Z=(Z(x_0),...,Z(x_N))$. On pourra alors calculer $L$, d'espérance $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions théoriques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La loi des grands nombres nous autorise à estimer l'espérance conditionnelle par la moyenne empirique de simulations conditionnelles. En effet, si on effectue $n$ simulations indépendantes de la longueur du plancher océanique $L_1,L_2,...,L_n$, la loi des grands nombres nous indique que si les $L_i$ sont de mêmes lois et de carrés intégrables, la variable aléatoire $M_n = \\dfrac{L_1+L_2+...+L_n}{n}$ converge presque surement et en moyenne vers $\\mathbb{E}(L)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après le cours \"Probabilité IV\", si on prend $Z=(Z_1,...,Z_N)$ gaussien  d'espérance $(\\mu,...,\\mu)$ et de matrice de covariance $C$ définie positive, $Y=(Z(x_{j_1}),...,Z(x_{j_n}))$ et $X$ contenant les éléments de $Z$ non présents dans $Y$.\n",
    "$f_{X|Y=y}(\\vec{x}) = \\dfrac{1}{(2\\pi)^{\\frac{N-n+2}{2}}}*\\dfrac{1}{\\sqrt{\\det{CS_X}}} * \\exp (-\\dfrac{1}{2}(x-\\psi (y))^TCS_X^{-1}(x-\\psi (y)) )$ où $CS_X = C_X-C_{X,Y}C_Y^{-1}C_{Y,X}$ et $\\psi (y) = (\\mu,...,\\mu) - C_{X,Y}C_Y^{-1}(y-(\\mu,...,\\mu))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $Y$ gaussien de moyenne $(0,...,0)$ et de variance $1$, de composantes indépendantes. Alors $\\Phi_Y(u)=\\exp (-\\dfrac{1}{2}u^2)$. Si $Z=m+RY$ avec $R$ une matrice et $m$ un vecteur : $\\Phi_Z(u)=e^{i<u,m>}\\Phi_Y(R^Tu) = e^{i<u,m>}e^{-\\frac{1}{2}(R^TR)(uu^T)}$ et donc $f_Z(z) = \\mathcal{F}^{-1}(\\Phi_Z)(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, il suffit de simuler $Z=\\mu+RY$ où $Y$ est de loi $\\mathcal{G}(0,1)$ et où $R$ est la décomposition de Cholesky de la matrice de covariance de $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions informatiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par initialiser les données du problème :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "A=0\n",
    "B=500\n",
    "N=101\n",
    "Delta= (B-A)/(N-1)\n",
    "discretization_indexes = np.arange(N)\n",
    "discretization = discretization_indexes*Delta\n",
    "\n",
    "\n",
    "mu=-5\n",
    "a=50\n",
    "sigma2=12\n",
    "\n",
    "observation_indexes = [0,20,40,60,80,100]\n",
    "depth = np.array([0,-4,-12.8,-1,-6.5,0])\n",
    "\n",
    "unknown_indexes=list(set(discretization_indexes)-set(observation_indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante permet d'obtenir la covariance en fonction de la distance entre deux points, on pourra aussi donner une matrice de distance pour obtenir la matrice de covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(d,a,v):\n",
    "    return v*np.exp(-np.abs(d)/a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons la matrice de distance des points de notre espace discrétisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(discretization):\n",
    "    n=len(discretization)\n",
    "    D=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            D[i][j]=np.abs(discretization[i]-discretization[j])\n",
    "    return D\n",
    "\n",
    "distance_matrix=distance(discretization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons ainsi la matrice de covariance de $Z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=covariance(distance_matrix,a,sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante prends en paramètre une matrice $M$ de taille $n \\times m$ et deux listes de tailles $n$ et $m$ (lines et columns) et renvoit la matrice extraite de $M$ en gardant uniquement les lignes et colonnes d'indices contenus dans lines et columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction(M,lines,columns):\n",
    "    L=M[columns]   #L est la matrice des lignes qu'on veut extraire\n",
    "    n=len(columns)\n",
    "    m=len(lines)\n",
    "    N=np.zeros((n,m))\n",
    "    for i in range(m):\n",
    "        N[i]=L[i][lines]\n",
    "    return N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons alors extraire la matrice de covariance entre les observations $C_X$, entre les observations et les inconnues $C_{X,Y}$ et entre les inconnues $C_Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cx=extraction(C,observation_indexes,observation_indexes)\n",
    "Cy=extraction(C,unknown_indexes,unknown_indexes)\n",
    "Cxy=extraction(C,observation_indexes,unknown_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
